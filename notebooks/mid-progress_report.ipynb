{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Speech Diarization with Offline Spectral Clustering\n",
    "\n",
    "## Table of Contents\n",
    "* [first-bullet](#first-bullet)\n",
    "* [Second Bullet Header](#second-bullet)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Scope\n",
    "\n",
    "#### What's inlcuded\n",
    "This project investigates and implements a spectral clustering algorithm in the context of a speech diarization pipeleine.\n",
    "\n",
    "This project tests the overall effectiveness of the pipeline before and after reimplementing the spectral clustering algorithm. Furthermore, this project extends the test cases beyond the initial project pipeline.\n",
    "\n",
    "#### What's not included\n",
    "This project does not reimplement pieces of an existing speech diarization pipeline (pyannote.pipeline) beyond what's necessary to allow for replacing the spectral clustering portion. This project does not investigate the running time or space complexity of the overall pipeline.\n",
    "\n",
    "### Clustering comparisons in literature\n",
    "\n",
    "Offline versus online clustering algorithms.\n",
    "\n",
    "K-means versus spectral clustering\n",
    "* K-means poor with non-gaussian data\n",
    "* Spectral overcomes with graph cut\n",
    "\n",
    "### Diarization Pipeline\n",
    "\n",
    "1. Speech Activity Detection (SAD)\n",
    "1. Extract short speech segments\n",
    "1. Create Compact Voice Vector: Embedding Extraction\n",
    "1. Cluster the segments and assign to a speaker\n",
    "1. (Optional) Resegmentation such as Variational Bayesian resegmentation\n",
    "\n",
    "## Spectral Clustering Analysis\n",
    "\n",
    "Create an eigen-decompose affinity matrix. Run k-means on voice vectors.\n",
    "Find the number of clusters using the max eigen-gap criteron\n",
    "\n",
    "## Test Plan\n",
    "\n",
    "### Spectral Clustering testing\n",
    "\n",
    "#### Expected results\n",
    "\n",
    "Reimplementation should perform better than publically availabile implementation, which differs from authors actual implementation.\n",
    "\n",
    "Training, dev, and eval datasets are not available and tuning parameter suggestions not provided.\n",
    "\n",
    "### Diarization Pipeline testing\n",
    "\n",
    "DER Metric\n",
    "\n",
    "pyannote.metrics library\n",
    "\n",
    "Datasets\n",
    "* CALLHOME\n",
    "* CALLHOME American English\n",
    "* 2003 NIST Rich Transcription (RT-03)\n",
    "  * Telephone Speech Part\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n",
    "Import a test file in the wav format. Convert from other formats as necessary, such as with ffmpeg (`ffmpeg -i *.mp3 *.wav`)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_file = {'uri': 'filename', 'audio': 'data/130126_009.wav'}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Speech Activity Detection (SAD)\n",
    "\n",
    "Use a pre-trained activity detection model from pyannote.audio"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyannote.audio.labeling.extraction import SequenceLabeling\n",
    "SAD_MODEL = ('lib/pyannote-audio/tutorials/models/speech_activity_detection/train/'\n",
    "             'AMI.SpeakerDiarization.MixHeadset.train/weights/0280.pt')\n",
    "\n",
    "sad = SequenceLabeling(model=SAD_MODEL)\n",
    "sad_scores = sad(test_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract Short Speech Segments <a class=\"anchor\" id=\"first-bullet\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyannote.audio.signal import Binarize\n",
    "binarize = Binarize(offset=0.94, onset=0.70, log_scale=True)\n",
    "speech = binarize.apply(sad_scores, dimension=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a Compact Voice Vector for Each Segment <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "\n",
    "Use a pre-trained model to detect when speakers are talking versus not talking."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyannote.audio.signal import Peak\n",
    "from pyannote.core import Timeline\n",
    "SCD_MODEL = ('lib/pyannote-audio/tutorials/models/speaker_change_detection/train/'\n",
    "             'AMI.SpeakerDiarization.MixHeadset.train/weights/0870.pt')\n",
    "\n",
    "scd = SequenceLabeling(model=SCD_MODEL)\n",
    "scd_scores = scd(test_file)\n",
    "peak = Peak(alpha=0.08, min_duration=0.40, log_scale=True)\n",
    "partition = peak.apply(scd_scores, dimension=1)\n",
    "speech_turns = partition.crop(speech)\n",
    "\n",
    "long_turns = Timeline(segments=[s for s in speech_turns if s.duration > 2.])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, extract an embeddded vector using a pretrained classifier for each segment."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyannote.audio.embedding.extraction import SequenceEmbedding\n",
    "EMB_MODEL = ('lib/pyannote-audio/tutorials/models/speaker_embedding/train/'               \n",
    "             'VoxCeleb.SpeakerVerification.VoxCeleb1.train/weights/2000.pt')\n",
    "emb = SequenceEmbedding(model=EMB_MODEL, duration=1., step=0.5)\n",
    "embeddings = emb(test_file)\n",
    "X, Y = [], []\n",
    "for segment in long_turns:\n",
    "    x = embeddings.crop(segment, mode='strict')\n",
    "    X.append(np.mean(x, axis=0))\n",
    "    y = test_file['annotation'].argmax(segment)\n",
    "    Y.append(y)\n",
    "    \n",
    "X = np.vstack(X)\n",
    "_, y_true = np.unique(Y, return_inverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Apply Spectral Clustering Algorithm to Identify Speakers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import lib.SpectralCluster.spectralcluster as sc\n",
    "clusterer =sc.spectral_clusterer.SpectralClusterer(\n",
    "            p_percentile=0.2,\n",
    "            gaussian_blur_sigma=0,\n",
    "            stop_eigenvalue=0.01)\n",
    "\n",
    "labels = clusterer.predict(X)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}